{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 â€” LoRA Fine-Tuning of SLM\n",
    "Align Small Language Model embeddings with Diffusion CLIP Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c2c27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusers: 0.27.2\n",
      "huggingface_hub: 0.24.6\n",
      "Torch CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import diffusers, huggingface_hub, torch\n",
    "print(\"diffusers:\", diffusers.__version__)\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "print(\"Torch CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peft'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionPipeline\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, TaskType\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch, pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:00<00:01,  4.02it/s]/home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading pipeline components...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:00<00:00,  4.31it/s]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLM & Diffusion Model Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "slm_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(slm_name)\n",
    "slm = AutoModelForCausalLM.from_pretrained(slm_name, torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "clip_encoder = pipe.text_encoder\n",
    "print(\"SLM & Diffusion Model Loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved\n"
     ]
    }
   ],
   "source": [
    "data = {\"caption\": [\n",
    "    \"a sunset over the ocean\",\n",
    "    \"a cat sitting on a laptop\",\n",
    "    \"a futuristic city skyline\",\n",
    "    \"a person walking in rain with umbrella\",\n",
    "] * 25}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"../data/mini_text_image_dataset.csv\", index=False)\n",
    "print(\"Dataset saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05)\n",
    "slm_lora = get_peft_model(slm, config)\n",
    "slm_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:02<00:00, 24.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Fine-tuning Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def dummy_train(slm, tokenizer, df):\n",
    "    optimizer = torch.optim.AdamW(slm.parameters(), lr=1e-4)\n",
    "    slm.train()\n",
    "    for caption in tqdm(df['caption'][:50], desc=\"Training\"):\n",
    "        inputs = tokenizer(caption, return_tensors=\"pt\", truncation=True, max_length=128).to(\"cuda\")\n",
    "        outputs = slm(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(\"LoRA Fine-tuning Complete\")\n",
    "dummy_train(slm_lora, tokenizer, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter saved to ../adapters/slm_lora_adapter/\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"../adapters/slm_lora_adapter/\"\n",
    "slm_lora.save_pretrained(output_dir)\n",
    "print(f\"LoRA adapter saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12341c2",
   "metadata": {},
   "source": [
    "Evaluation MAtrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90140e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cse-sdpl/anaconda3/envs/gpu_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f5b352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27.2\n"
     ]
    }
   ],
   "source": [
    "import diffusers\n",
    "print(diffusers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0111ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c91dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: ./adapters/tinyllama_magicbrush_lora\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create folder for LoRA adapter\n",
    "lora_dir = \"./adapters/tinyllama_magicbrush_lora\"\n",
    "os.makedirs(lora_dir, exist_ok=True)\n",
    "print(\"Created:\", lora_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LoraConfig' from 'diffusers' (/home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages/diffusers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DDPMScheduler\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoencoderKL\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DPMSolverMultistepScheduler\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LoraConfig' from 'diffusers' (/home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages/diffusers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import UNet2DConditionModel\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers import LoraConfig\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "print(\"Loading base Stable Diffusion model...\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Small dummy dataset (one image)\n",
    "os.makedirs(\"dummy_train\", exist_ok=True)\n",
    "Image.new(\"RGB\", (512,512), color=\"white\").save(\"dummy_train/dummy.png\")\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoRAConfig(\n",
    "    r=4,\n",
    "    lora_alpha=4,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"to_q\", \"to_v\"]\n",
    ")\n",
    "\n",
    "# Apply LoRA to UNet\n",
    "pipe.unet.add_adapter(lora_config)\n",
    "\n",
    "# VERY small training loop\n",
    "optimizer = torch.optim.Adam(pipe.unet.parameters(), lr=1e-4)\n",
    "\n",
    "print(\"Training a tiny LoRA adapter (10 steps only)...\")\n",
    "\n",
    "for step in range(10):\n",
    "    img = Image.open(\"dummy_train/dummy.png\")\n",
    "\n",
    "    latents = pipe.vae.encode(pipe.image_processor(img).unsqueeze(0).to(device)).latent_dist.sample()\n",
    "    noise = torch.randn_like(latents)\n",
    "    timesteps = torch.randint(0, 1000, (1,), device=device).long()\n",
    "\n",
    "    noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "    model_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=torch.randn(1,77,768).to(device)).sample\n",
    "    loss = torch.nn.functional.mse_loss(model_pred, noise)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Step {step+1}/10 | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save LoRA weights\n",
    "pipe.unet.save_attn_procs(\"./adapters/tinyllama_magicbrush_lora\")\n",
    "print(\"âœ” Dummy LoRA saved at adapters/tinyllama_magicbrush_lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14da00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Generating BEFORE LoRA image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:00<00:01,  3.44it/s]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  6.94it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:02<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs/before_lora.png\n",
      "â–¶ Generating AFTER LoRA image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:00<00:01,  3.18it/s]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Loading LoRA adapter from local directory...\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': './adapters/tinyllama_magicbrush_lora'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 61\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâ–¶ Loading LoRA adapter from local directory...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m lora_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./adapters/tinyllama_magicbrush_lora\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# <-- ensure this exists!\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mpipe_lora\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lora_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoRA loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m after_img \u001b[38;5;241m=\u001b[39m pipe_lora(prompt, num_inference_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.10/site-packages/diffusers/loaders/lora.py:106\u001b[0m, in \u001b[0;36mLoraLoaderMixin.load_lora_weights\u001b[0;34m(self, pretrained_model_name_or_path_or_dict, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mLoad LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m`self.text_encoder`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m        `default_{i}` where i is the total number of adapters being loaded.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# First, ensure that the checkpoint is a compatible one and can be successfully loaded.\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m state_dict, network_alphas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m is_correct_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlora\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_correct_format:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.10/site-packages/diffusers/loaders/lora.py:231\u001b[0m, in \u001b[0;36mLoraLoaderMixin.lora_state_dict\u001b[0;34m(cls, pretrained_model_name_or_path_or_dict, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Here we're relaxing the loading check to enable more Inference API\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# friendliness where sometimes, it's not at all possible to automatically\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# determine `weight_name`.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m         weight_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_best_guess_weight_name\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.safetensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     model_file \u001b[38;5;241m=\u001b[39m _get_model_file(\n\u001b[1;32m    235\u001b[0m         pretrained_model_name_or_path_or_dict,\n\u001b[1;32m    236\u001b[0m         weights_name\u001b[38;5;241m=\u001b[39mweight_name \u001b[38;5;129;01mor\u001b[39;00m LORA_WEIGHT_NAME_SAFE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    246\u001b[0m     )\n\u001b[1;32m    247\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m safetensors\u001b[38;5;241m.\u001b[39mtorch\u001b[38;5;241m.\u001b[39mload_file(model_file, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.10/site-packages/diffusers/loaders/lora.py:307\u001b[0m, in \u001b[0;36mLoraLoaderMixin._best_guess_weight_name\u001b[0;34m(cls, pretrained_model_name_or_path_or_dict, file_extension)\u001b[0m\n\u001b[1;32m    303\u001b[0m     targeted_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    304\u001b[0m         f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(pretrained_model_name_or_path_or_dict) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(file_extension)\n\u001b[1;32m    305\u001b[0m     ]\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     files_in_repo \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msiblings\n\u001b[1;32m    308\u001b[0m     targeted_files \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mrfilename \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files_in_repo \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mrfilename\u001b[38;5;241m.\u001b[39mendswith(file_extension)]\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(targeted_files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './adapters/tinyllama_magicbrush_lora'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load CLIP for Evaluation\n",
    "# ------------------------------\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def compute_clip_score(text, image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(text=[text], images=img, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = clip_model(**inputs)\n",
    "        logits = out.logits_per_image.item()\n",
    "        img_emb = out.image_embeds[0]\n",
    "        txt_emb = out.text_embeds[0]\n",
    "        l2_dist = torch.nn.functional.pairwise_distance(img_emb.unsqueeze(0), txt_emb.unsqueeze(0)).item()\n",
    "\n",
    "    return logits, l2_dist\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Generate Base Model Image\n",
    "# ------------------------------\n",
    "print(\"â–¶ Generating BEFORE LoRA image...\")\n",
    "\n",
    "pipe_base = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "prompt = \"A serene sunset over a calm lake with orange sky and reflections.\"\n",
    "\n",
    "before_img = pipe_base(prompt, num_inference_steps=25).images[0]\n",
    "before_path = \"outputs/before_lora.png\"\n",
    "before_img.save(before_path)\n",
    "\n",
    "print(\"Saved:\", before_path)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Generate LoRA Fine-Tuned Image\n",
    "# ------------------------------\n",
    "print(\"â–¶ Generating AFTER LoRA image...\")\n",
    "\n",
    "pipe_lora = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Load your LoRA adapter\n",
    "\n",
    "print(\"â–¶ Loading LoRA adapter from local directory...\")\n",
    "\n",
    "lora_dir = \"./adapters/tinyllama_magicbrush_lora\"  # <-- ensure this exists!\n",
    "\n",
    "pipe_lora.load_lora_weights(\n",
    "    lora_dir,\n",
    "    use_safetensors=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"LoRA loaded successfully.\")\n",
    "\n",
    "after_img = pipe_lora(prompt, num_inference_steps=25).images[0]\n",
    "after_path = \"outputs/after_lora.png\"\n",
    "after_img.save(after_path)\n",
    "\n",
    "print(\"Saved:\", after_path)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Compute CLIP Similarity\n",
    "# ------------------------------\n",
    "print(\"\\nâ–¶ Computing CLIP scores ...\")\n",
    "\n",
    "before_score, before_l2 = compute_clip_score(prompt, before_path)\n",
    "after_score, after_l2   = compute_clip_score(prompt, after_path)\n",
    "\n",
    "print(\"\\nðŸ“Š Final Comparison\")\n",
    "print(\"----------------------------\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nBefore LoRA:\")\n",
    "print(f\" - CLIP Similarity: {before_score:.4f}\")\n",
    "print(f\" - L2 Distance:     {before_l2:.4f}\")\n",
    "\n",
    "print(\"\\nAfter LoRA:\")\n",
    "print(f\" - CLIP Similarity: {after_score:.4f}\")\n",
    "print(f\" - L2 Distance:     {after_l2:.4f}\")\n",
    "\n",
    "improvement = after_score - before_score\n",
    "print(\"\\nâœ¨ Improvement (Î” CLIP Score):\", round(improvement, 4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
