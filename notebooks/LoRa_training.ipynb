{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a822874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft==0.11.1 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (0.11.1)\n",
      "Requirement already satisfied: transformers==4.36.2 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (4.36.2)\n",
      "Requirement already satisfied: accelerate==0.25.0 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from peft==0.11.1) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from peft==0.11.1) (25.0)\n",
      "Requirement already satisfied: psutil in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from peft==0.11.1) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from peft==0.11.1) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from peft==0.11.1) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from peft==0.11.1) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from peft==0.11.1) (0.6.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from peft==0.11.1) (0.24.6)\n",
      "Requirement already satisfied: filelock in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from transformers==4.36.2) (3.20.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from transformers==4.36.2) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from transformers==4.36.2) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from transformers==4.36.2) (0.15.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (4.15.0)\n",
      "Requirement already satisfied: networkx in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.11.1) (12.9.86)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.11.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from requests->transformers==4.36.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from requests->transformers==4.36.2) (2025.11.12)\n",
      "Requirement already satisfied: diffusers==0.24.0 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (0.24.0)\n",
      "Requirement already satisfied: Pillow in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from diffusers==0.24.0) (12.0.0)\n",
      "Requirement already satisfied: filelock in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from diffusers==0.24.0) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from diffusers==0.24.0) (0.24.6)\n",
      "Requirement already satisfied: importlib-metadata in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from diffusers==0.24.0) (8.7.0)\n",
      "Requirement already satisfied: numpy in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from diffusers==0.24.0) (2.2.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from diffusers==0.24.0) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from diffusers==0.24.0) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from diffusers==0.24.0) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->diffusers==0.24.0) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->diffusers==0.24.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->diffusers==0.24.0) (6.0.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->diffusers==0.24.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->diffusers==0.24.0) (4.15.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.24.0) (3.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from requests->diffusers==0.24.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from requests->diffusers==0.24.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from requests->diffusers==0.24.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cse-sdpl/anaconda3/envs/gpu_env/lib/python3.10/site-packages (from requests->diffusers==0.24.0) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft==0.11.1 transformers==4.36.2 accelerate==0.25.0 --no-cache-dir\n",
    "!pip install diffusers==0.24.0 --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "041d3e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7be35d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  8.66it/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "prompt = \"A cute cat sitting on Mars wearing a space helmet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b14149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved before image: outputs/before_lora.png\n"
     ]
    }
   ],
   "source": [
    "before_img = pipe(prompt, num_inference_steps=25).images[0]\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "before_path = \"outputs/before_lora.png\"\n",
    "before_img.save(before_path)\n",
    "\n",
    "print(\"Saved before image:\", before_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddee13cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered LoRA-compatible modules:\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "  - down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "  - down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "  - down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "  - down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "  - down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "  - down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "  - up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "  - up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.1\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0\n",
      "  - up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.1\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "  - up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "  - up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.1\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0\n",
      "  - up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.1\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "  - up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "  - up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.1\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0\n",
      "  - up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.1\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "  - mid_block.attentions.0.transformer_blocks.0.attn2.to_out.1\n"
     ]
    }
   ],
   "source": [
    "# Find valid LoRA injection points inside SD 1.5 UNet\n",
    "target_modules = []\n",
    "\n",
    "for name, module in pipe.unet.named_modules():\n",
    "    if any(x in name for x in [\"to_q\", \"to_k\", \"to_v\", \"to_out\"]):\n",
    "        target_modules.append(name)\n",
    "\n",
    "print(\"Discovered LoRA-compatible modules:\")\n",
    "for m in target_modules:\n",
    "    print(\"  -\", m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7ff2ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  6.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch, os\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd7399e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21819/3635222447.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lora = torch.load(\"adapters/tinyllama_magicbrush_lora/unet_lora.pth\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'adapters/tinyllama_magicbrush_lora/unet_lora.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionPipeline\n\u001b[0;32m----> 4\u001b[0m lora \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madapters/tinyllama_magicbrush_lora/unet_lora.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m StableDiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunwayml/stable-diffusion-v1-5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m      9\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m unet \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39munet\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.10/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.10/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_env/lib/python3.10/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'adapters/tinyllama_magicbrush_lora/unet_lora.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "lora = torch.load(\"adapters/tinyllama_magicbrush_lora/unet_lora.pth\")\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "unet = pipe.unet\n",
    "\n",
    "# Inject LoRA into UNet\n",
    "for name, module in unet.named_modules():\n",
    "    if name in lora:\n",
    "        A, B = lora[name]\n",
    "        module.weight.data += (B @ A) * 0.1   # scale factor\n",
    "\n",
    "print(\"LoRA loaded!\")\n",
    "\n",
    "prompt = \"a sunset over a lake\"\n",
    "output = pipe(prompt, num_inference_steps=25).images[0]\n",
    "output.save(\"after_lora.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49f55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
